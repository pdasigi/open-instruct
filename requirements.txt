torch<=2.0.1
scipy
packaging
sentencepiece
datasets<=2.17.0  # 2.18.0 throws many warnings and caching seems not working in distributed setup
deepspeed>=0.10.0
accelerate>=0.21.0,<0.23.0  # 0.23.0 will cause an incorrect learning rate schedule when using deepspeed, which is likely caused by https://github.com/huggingface/accelerate/commit/727d624322c67db66a43c559d8c86414d5ffb537
peft>=0.4.0
bitsandbytes>=0.41.1
evaluate>=0.4.0
tokenizers>=0.13.3
protobuf
transformers
openai>=1.0.0
tiktoken
rouge_score
tensorboard
wandb
gradio==3.50.2
termcolor
jsonlines
unidic-lite
einops
flash-attn==2.2.2
auto-gptq
fire
alpaca_eval==0.5.3
# for human eval web app
flask
vllm 
openpyxl
# for ifeval
nltk
langdetect
immutabledict
